import re
import yaml
import feedparser
import socket
from datetime import datetime, timedelta, timezone
from dateutil import parser as dateparser
def clean_title(s: str):
    if not s:
        return ""
    return " ".join(s.split())


def load_keywords(path: str):
    kws = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            k = line.strip()
            if k and not k.startswith("#"):
                kws.append(k.lower())
    return kws


def text_match(text: str, keywords):
    t = (text or "").lower()
    return any(k in t for k in keywords)


def parse_dt(entry):
    for k in ("published", "updated", "created"):
        if k in entry and entry[k]:
            try:
                return dateparser.parse(entry[k])
            except Exception:
                pass
    for k in ("published_parsed", "updated_parsed"):
        if k in entry and entry[k]:
            try:
                return datetime(*entry[k][:6], tzinfo=timezone.utc)
            except Exception:
                pass
    return None


    



def fetch_items(feed_urls, keywords, start_dt, end_dt, limit=10):
    items = []
    seen = set()

    for url in feed_urls:
        socket.setdefaulttimeout(10)    
        try:
            d = feedparser.parse(url)
        except Exception:
            continue    

        entries = getattr(d, "entries", None) or []
        for e in entries[:50]:
            title = clean_title(getattr(e, "title", ""))
            link = getattr(e, "link", "")
            summary = getattr(e, "summary", "") or getattr(e, "description", "")

            if not title or not link:
                continue

            dt = parse_dt(e)
            if dt is not None:
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                dt_utc = dt.astimezone(timezone.utc)
                if not (start_dt <= dt_utc < end_dt):
                    continue

            if not (text_match(title, keywords) or text_match(summary, keywords)):
                continue

            key = title.lower()
            if key in seen:
                continue
            seen.add(key)

            items.append((dt or datetime(1970, 1, 1, tzinfo=timezone.utc), title, link))

    items.sort(key=lambda x: x[0], reverse=True)
    return items[:limit]


def main():
    # æ—¶é—´çª—å£ï¼šåŒ—äº¬æ—¶é—´æ˜¨å¤©
    tz_bj = timezone(timedelta(hours=8))
    now_bj = datetime.now(tz_bj)
    yday_bj = (now_bj - timedelta(days=1)).date()

    start_bj = datetime(yday_bj.year, yday_bj.month, yday_bj.day, 0, 0, 0, tzinfo=tz_bj)
    end_bj = start_bj + timedelta(days=1)
    start_utc = start_bj.astimezone(timezone.utc)
    end_utc = end_bj.astimezone(timezone.utc)

    # âœ… è¿™é‡Œæ˜¯ feeds çš„å”¯ä¸€å®šä¹‰ä½ç½®ï¼ˆéå¸¸å…³é”®ï¼‰
    with open("config/feeds.yaml", "r", encoding="utf-8") as f:
        feeds = yaml.safe_load(f) or {}

    keywords = load_keywords("config/keywords.txt")

    global_items = fetch_items(
        feeds.get("global", []),
        keywords,
        start_utc,
        end_utc,
        limit=6
    )

    china_items = fetch_items(
        feeds.get("china", []),
        keywords,
        start_utc,
        end_utc,
        limit=6
    )

    # å…œåº•ï¼šå¦‚æœä¸­å›½æ²¡æœ‰å‘½ä¸­å…³é”®è¯ï¼Œå–æœ€æ–° 3 æ¡
    if not china_items:
        china_items = fetch_items(
            feeds.get("china", []),
            [" "],
            start_utc,
            end_utc,
            limit=3
        )

    today_str = now_bj.strftime("%Y-%m-%d")
    yday_str = yday_bj.strftime("%Y-%m-%d")

    def fmt(items):
        if not items:
            return ["ï¼ˆæœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°é—»ï¼‰"]
        out = []
        for i, (_, title, link) in enumerate(items, 1):
            out.append(f"{i}. {title}\n{link}")
        return out

    msg = []
    msg.append(f"ğŸ¤– AIç¡¬ä»¶æ—¥æŠ¥ï½œ{today_str}ï¼ˆæŠ“å– {yday_str}ï¼‰")
    msg.append("")
    msg.append("ğŸŒ å…¨çƒ")
    msg.extend(fmt(global_items))
    msg.append("")
    msg.append("ğŸ‡¨ğŸ‡³ ä¸­å›½")
    msg.extend(fmt(china_items))
    msg.append("")
    msg.append("ğŸ“Œ è¯´æ˜ï¼šæœ¬æ—¥æŠ¥ä¸º RSS + å…³é”®è¯ç­›é€‰ï¼ˆåŠè‡ªåŠ¨ï¼‰ã€‚")

    
    print("\n".join(msg))   # âœ… å”¯ä¸€ print


from datetime import datetime
today_str = datetime.now().strftime("%Y-%m-%d")

global_items = []
china_items = []

# è¿™é‡Œæ˜¯ä½ æŠ“ RSSã€ç­›å…³é”®è¯ã€append title çš„é€»è¾‘

    

# ===== Generate Xiaohongshu-style daily (viral structure) =====
def pick(items, n=3):
    # å–å‰ n æ¡ï¼Œé¿å…ç©º
    return items[:n] if items else []

top_global = pick(global_items, 3)
top_china = pick(china_items, 3)

# ç»„åˆä»Šæ—¥Top3ï¼ˆä¼˜å…ˆå…¨çƒ+ä¸­å›½æ··åˆï¼‰
top3 = (top_global + top_china)[:3]

# è‡ªåŠ¨ç”Ÿæˆâ€œ3å¥ç»“è®ºâ€ï¼ˆä¸æ¥AIï¼Œè§„åˆ™åŒ–ä½†å¾ˆåƒçœŸäººï¼‰
conclusions = []
if len(global_items) + len(china_items) >= 8:
    conclusions.append("â‘  ä¿¡æ¯å¯†åº¦å¾ˆé«˜ï¼šç¡¬ä»¶/ä¾›åº”é“¾ç›¸å…³æ›´æ–°æ˜æ˜¾åŠ é€Ÿã€‚")
else:
    conclusions.append("â‘  ä»Šå¤©ä¸ç®—çˆ†é‡ï¼Œä½†ä¸»çº¿ä¾ç„¶æ¸…æ™°ï¼šç®—åŠ›ä¸ä¾›åº”é“¾ã€‚")

if len(china_items) >= 3:
    conclusions.append("â‘¡ ä¸­å›½ç›¸å…³åŠ¨æ€å æ¯”ä¸Šæ¥äº†ï¼šä¸åªæ˜¯è·Ÿè¿›ï¼Œä¹Ÿå¼€å§‹â€œå®šèŠ‚å¥â€ã€‚")
else:
    conclusions.append("â‘¡ æµ·å¤–ä»æ›´æ´»è·ƒï¼šæ–°å“/æ•°æ®ä¸­å¿ƒ/èŠ¯ç‰‡ä»åœ¨æŠ¢å¤´æ¡ã€‚")

conclusions.append("â‘¢ ç»“è®ºï¼šAI ç¡¬ä»¶ç«äº‰å·²ä»â€œå•ç‚¹æ€§èƒ½â€è½¬å‘â€œç³»ç»Ÿä¸ç”Ÿæ€â€ã€‚")

# 3ä¸ªçœ‹ç‚¹ï¼šç›´æ¥å¼•ç”¨ä»Šæ—¥Top3æ ‡é¢˜ï¼ˆæ›´çœŸå®ã€æ›´çœäº‹ï¼‰
highlights = []
for i, t in enumerate(top3, 1):
    highlights.append(f"{i}. {t}")

# ä¸€å¥é‡‘å¥ï¼ˆå›ºå®šä½†å¾ˆåƒæ ç›®sloganï¼‰
golden = "é‡‘å¥ï¼šç®—åŠ›ä¸æ˜¯æœªæ¥ï¼Œç®—åŠ›â€œè¢«æ­£ç¡®ä½¿ç”¨â€æ‰æ˜¯æœªæ¥ã€‚"

# Hashtagsï¼ˆå›ºå®š+è½»é‡ï¼Œé¿å…å¤ªâ€œè¥é”€â€ï¼‰
tags = "#AIç¡¬ä»¶ #èŠ¯ç‰‡ #ç®—åŠ› #æ•°æ®ä¸­å¿ƒ #æœºå™¨äºº #ç§‘æŠ€èµ„è®¯ #æ¯æ—¥èµ„è®¯"

xhs = []
# æ ‡é¢˜ï¼šçŸ­ã€å¼ºã€å¸¦æ—¥æœŸ
xhs.append(f"ğŸ“Œ AIç¡¬ä»¶æ—¥æŠ¥ï½œ{today_str}ï½œä»Šå¤©3ä¸ªä¿¡å·")
xhs.append("")
# å¼€å¤´é’©å­
xhs.append("ä»Šå¤©çš„ AI ç¡¬ä»¶åœˆï¼Œæˆ‘åªæƒ³è¯´ï¼šåˆ«ç›¯ç€å‚æ•°è¡¨ï¼ŒçœŸæ­£çš„æˆ˜åœºæ˜¯â€œç³»ç»Ÿâ€ã€‚")
xhs.append("")
# 3å¥ç»“è®º
xhs.append("âœ… 3å¥ç»“è®ºï¼ˆå…ˆçœ‹è¶‹åŠ¿ï¼‰")
for c in conclusions:
    xhs.append(c)
xhs.append("")
# 3ä¸ªçœ‹ç‚¹
xhs.append("ğŸ”¥ 3ä¸ªçœ‹ç‚¹ï¼ˆä»Šå¤©æœ€å€¼å¾—ç‚¹å¼€ï¼‰")
if highlights:
    xhs.extend(highlights)
else:
    xhs.append("1. ä»Šå¤©æŠ“åˆ°çš„æœ‰æ•ˆæ–°é—»è¾ƒå°‘ï¼ˆå»ºè®®æ”¾å®½å…³é”®è¯æˆ–å¢åŠ RSSæºï¼‰ã€‚")
xhs.append("")
# æµ·å¤–/ä¸­å›½åˆ†åŒºï¼ˆç»™å–œæ¬¢ä¿¡æ¯å¯†åº¦çš„äººï¼‰
xhs.append("ğŸŒ æµ·å¤– AI ç¡¬ä»¶åŠ¨æ€ï¼ˆç²¾é€‰ï¼‰")
if global_items:
    for i, t in enumerate(global_items[:5], 1):
        xhs.append(f"{i}ï¸âƒ£ {t}")
else:
    xhs.append("- ä»Šå¤©æ²¡æŠ“åˆ°ç¬¦åˆå…³é”®è¯çš„æµ·å¤–æ–°é—»ï¼Œå¯åœ¨ config/feeds.yaml æ”¾å®½å…³é”®è¯æˆ–åŠ æºã€‚")
xhs.append("")
xhs.append("ğŸ‡¨ğŸ‡³ ä¸­å›½ AI ç¡¬ä»¶è§‚å¯Ÿï¼ˆç²¾é€‰ï¼‰")
if china_items:
    for i, t in enumerate(china_items[:5], 1):
        xhs.append(f"{i}ï¸âƒ£ {t}")
else:
    xhs.append("- ä»Šå¤©æ²¡æŠ“åˆ°ç¬¦åˆå…³é”®è¯çš„ä¸­å›½æ–°é—»ï¼Œå¯åœ¨ config/feeds.yaml æ”¾å®½å…³é”®è¯æˆ–åŠ æºã€‚")
xhs.append("")
# é‡‘å¥ + äº’åŠ¨
xhs.append(f"ğŸ’¬ {golden}")
xhs.append("")
xhs.append("ğŸ—£ï¸ äº’åŠ¨ï¼šä½ æ›´çœ‹å¥½å“ªæ¡èµ›é“ï¼Ÿï¼ˆ1ï¼‰èŠ¯ç‰‡ï¼ˆ2ï¼‰æœºå™¨äººï¼ˆ3ï¼‰ç«¯ä¾§AIï¼ˆ4ï¼‰æ•°æ®ä¸­å¿ƒ")
xhs.append(tags)

with open("daily_xhs.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(xhs) + "\n")


 


if __name__ == "__main__":
    main()
